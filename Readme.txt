This is Scala codes with Spark to search target word and find the document with highest TF score and corresponding TF score. main() will call getTF() to create a RDD(named as joined in the codes) in this format: (word, List((fileName1, TermFrequencyScore),(fileName2, TermFrequencyScore)...)). Using this RDD, we will get the docuemnt with highest TF score and corresponding TF score.

I made it a Maven project, so I think to run this codes, you just need to pull it from GitHub and run TermFrequency.scala. The sample documents are in data folder. The output result is in result.txt.

Chitty chatty digression:
I used to write java and python with Elastic Search, but noticing this is not a difficult task, I decided to learn Scala and Spark in these two days and tried to write the codes to implement the function. I was just surprised by how powerful and elegant Scala and Spark can be, so I am changing one current code for document classification in my current part-time work to use Scala and Spark. Quite interesting process, I learned a lot from this experience and impoved another project on hand, just wanna say thank you.